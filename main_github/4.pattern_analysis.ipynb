{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoolQ - BERT - Main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import io\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for CNN\n",
    "from torchvision import transforms\n",
    "input_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=input_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 4\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolutional Layer 5\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Adjusted size calculation after 5 pooling layers\n",
    "        size_after_conv = input_size // 32  # Each pooling layer halves the dimension\n",
    "        self.fc1 = nn.Linear(256 * size_after_conv * size_after_conv, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = self.pool3(self.act3(self.conv3(x)))\n",
    "        x = self.pool4(self.act4(self.conv4(x)))\n",
    "        x = self.pool5(self.act5(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"boolq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pairs = [[row['question'], row['passage']] for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model link and tokenizer\n",
    "model_link = \"rycecorn/distil-bert-fine-tuned-boolq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_link)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_link, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_attention_matrix(input_sentence, attention_head):\n",
    "    test = input_pairs[input_sentence]\n",
    "    tokenized_input = tokenizer(test[0], test[1], truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**tokenized_input)\n",
    "    attention_outputs = torch.stack(outputs.attentions)\n",
    "    flattened_attention_matrices = attention_outputs.view(-1, attention_outputs.size(3), attention_outputs.size(4))\n",
    "    flattened_attention_matrices = flattened_attention_matrices\n",
    "    \n",
    "    selected_attention_head = flattened_attention_matrices[attention_head].cpu().numpy()\n",
    "    \n",
    "      # Clear intermediate variables\n",
    "    del test, tokenized_input, outputs, attention_outputs, flattened_attention_matrices\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return selected_attention_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_head_pattern(attention_pattern):\n",
    "    # plot attention pattern\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_pattern, cmap='magma', interpolation='nearest')\n",
    "    \n",
    "    #Convert to bytes and to image file\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    png_variable = buf.getvalue()\n",
    "    image_buffer = io.BytesIO(png_variable)\n",
    "    \n",
    "    # Close buffer - delete from memory\n",
    "    buf.close()\n",
    "    \n",
    "    # Prep image for CNN\n",
    "    img = PIL.Image.open(image_buffer)\n",
    "    imgCropped = img.crop(box= (205, 96, 820, 713))\n",
    "    enhancer = PIL.ImageEnhance.Contrast(imgCropped)\n",
    "    enhanced_image = enhancer.enhance(4.0)\n",
    "    gray_image = enhanced_image.convert(\"L\")\n",
    "    img = transform(gray_image.convert('RGB'))\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # Clear intermediate variables and collect garbage\n",
    "    del attention_pattern, png_variable, imgCropped, enhancer, enhanced_image, gray_image\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = torch.load('ahr_cnn_75_acc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_number = random.randint(0, 999)\n",
    "head_number = random.randint(0, 73)\n",
    "print(f'Input num: {sentence_number}')\n",
    "print(f'Head num: {head_number}')\n",
    "\n",
    "head_check = get_self_attention_matrix(sentence_number, head_number)\n",
    "test_img = classify_head_pattern(head_check)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_cnn(test_img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f'Predicted class: {predicted.item()}') # Labels for classifier are range 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_to_prune = [34, 42, 29, 60, 59, 58, 43, 40, 44, 55, 53, 27, 51, 48, 50, 25, 45, 33, 49, 38, 52, 54, 31, 56, 39, 32, 36, 35, 37, 57, 28, 47, 63, 46, 70, 30, 16, 26, 15, 18, 64, 7, 68, 8, 71, 20, 62, 5, 24, 72, 66, 61, 12, 14, 6, 65, 67, 17, 1, 69, 23, 41, 10, 21, 9, 2, 19, 4]\n",
    "attention_pattern_dict = {i: [0, 0, 0, 0, 0] for i in range(1, 73)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
