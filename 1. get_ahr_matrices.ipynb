{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below can be used to generate the Attention Head Redundancy (AHR) matrices to be used for the next steps. \n",
    "Things to that need to be modified to replicate the experiments on different tasks/models:\n",
    "<ol>\n",
    "<li> Task and model (loaded using HuggingFace) </li>\n",
    "<li> Number of heads in the AHR visualization </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\datbi\\anaconda3\\envs\\thesis-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"boolq\") # Select dataset for experiments\n",
    "input_pairs = [[row['question'], row['passage']] for row in dataset['train']] # Change to fit the input sequence of the chosen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes average cosine similarity for each input\n",
    "def compute_distances_for_all_pairs(attention_matrices):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarities across the attention matrices generated for a singular input\n",
    "\n",
    "    Args:\n",
    "    - inputs atention matrix.\n",
    "\n",
    "    Returns:\n",
    "    - Similarity scores for all pairs of attention heads in the input (expected no_of_attention_head_pairs C 2).\n",
    "    \"\"\"\n",
    "    # Assume attention_matrices is of shape (144, n, n)\n",
    "    num_attention_matrices, n_tokens, _ = attention_matrices.shape\n",
    "    distances = []\n",
    "\n",
    "    for i in range(num_attention_matrices):\n",
    "        for j in range(i + 1, num_attention_matrices):\n",
    "            total_distance = 0\n",
    "            for token_idx in range(n_tokens):\n",
    "                vector_a = attention_matrices[i, token_idx, :].flatten()\n",
    "                vector_b = attention_matrices[j, token_idx, :].flatten()\n",
    "\n",
    "                # Compute cosine similarity\n",
    "                dot_product = np.dot(vector_a, vector_b)\n",
    "                norm_a = np.linalg.norm(vector_a)\n",
    "                norm_b = np.linalg.norm(vector_b)\n",
    "                similarity = dot_product / (norm_a * norm_b)\n",
    "\n",
    "                total_distance += similarity\n",
    "\n",
    "            avg_distance = total_distance / n_tokens\n",
    "            distances.append(avg_distance)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all input pairs and extract attention matrix\n",
    "def compute_cosine_sim_for_all_inputs(inputs_list):\n",
    "  all_avg_similarities = []\n",
    "  for input_pairs in tqdm(inputs_list, desc='Pairs of inputs'):\n",
    "    tokenized_input = tokenizer(input_pairs[0], input_pairs[1], truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**tokenized_input)\n",
    "\n",
    "    attention_outputs = torch.stack(outputs.attentions)\n",
    "    #attention_outputs = attention_outputs.cpu()\n",
    "    flattened_attention_matrices = attention_outputs.view(-1, attention_outputs.size(3), attention_outputs.size(4))\n",
    "\n",
    "    final_scalar_values = compute_distances_for_all_pairs(flattened_attention_matrices.numpy())\n",
    "    all_avg_similarities.append(final_scalar_values)\n",
    "\n",
    "  return all_avg_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models from HF, the models used below are custom fine-tuned models on the BoolQ task\n",
    "\n",
    "# Fine-tuned BERT BoolQ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rycecorn/bert-fine-tuned-boolq\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rycecorn/bert-fine-tuned-boolq\", output_attentions=True)\n",
    "\n",
    "'''# Fine-tuned DistilBERT\n",
    "model_link = \"rycecorn/distil-bert-fine-tuned-boolq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_link)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_link, output_attentions=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the AHR matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahr_matrices = compute_cosine_sim_for_all_inputs(input_pairs[:1000]) # change to desired number of input\n",
    "## output has dim (N, num_pairs)\n",
    "\n",
    "# Saving the input to visualize the AHR matrices\n",
    "with open('./outputs/attention_head_redundancy/boolq_cosine_sim_BERT_1000.pkl', 'wb') as file:\n",
    "    pickle.dump(ahr_matrices, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHR visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average redundancies across all pairs\n",
    "average_correlation_across_inputs = [sum(values) / len(values) for values in zip(*ahr_matrices)]\n",
    "\n",
    "# Number of attention heads\n",
    "num_heads = 144 # Change to 72 for DistilBERT variants\n",
    "\n",
    "# Convert distances to similarities\n",
    "# Example using exponential decay, adjust based on your preference\n",
    "similarities = np.exp(-np.array(average_correlation_across_inputs))\n",
    "\n",
    "# Initialize the redundancy matrix with zeros\n",
    "redundancy_matrix = np.zeros((num_heads, num_heads))\n",
    "\n",
    "# Fill the redundancy matrix\n",
    "k = 0\n",
    "for i in range(num_heads):\n",
    "    for j in range(i+1, num_heads):\n",
    "        redundancy_matrix[i, j] = similarities[k]\n",
    "        redundancy_matrix[j, i] = similarities[k]  # Symmetric\n",
    "        k += 1\n",
    "\n",
    "# Set the diagonal to the maximum similarity score\n",
    "np.fill_diagonal(redundancy_matrix, 1)\n",
    "\n",
    "\n",
    "# Visualizing the attention redundancy matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(redundancy_matrix, cmap='viridis', annot=False, square=True, cbar_kws={'shrink': .5})\n",
    "plt.title('Attention Redundancy Matrix for BoolQ - Cosine similarity / 1000 inputs / BERT-base')\n",
    "plt.xlabel('Attention Head')\n",
    "plt.ylabel('Attention Head')\n",
    "plt.savefig('./outputs/attention_head_redundancy/ahr_boolq_cosine_sim_BERT-Base-BoolQ_1000.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
