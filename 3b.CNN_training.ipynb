{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('image_output.csv') # Change to your csv file with the file_paths and labels\n",
    "data['file_path'] = data['file_path'].apply(lambda x: os.path.basename(x))\n",
    "data['label'] = data['label'].replace(0, 5)\n",
    "data['label'] = data['label'].astype(str)\n",
    "data.rename(columns = {'file_path':'filename'}, inplace = True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cnn_filename_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "input_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform = None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.iloc[index][\"filename\"]\n",
    "        label = self.df.iloc[index][\"label\"] - 1\n",
    "        \n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename)).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"cnn_filename_labels.csv\", \"./image_outputs\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=input_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 4\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolutional Layer 5\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Adjusted size calculation after 5 pooling layers\n",
    "        size_after_conv = input_size // 32  # Each pooling layer halves the dimension\n",
    "        self.fc1 = nn.Linear(256 * size_after_conv * size_after_conv, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = self.pool3(self.act3(self.conv3(x)))\n",
    "        x = self.pool4(self.act4(self.conv4(x)))\n",
    "        x = self.pool5(self.act5(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SimpleCNN(num_classes=5, input_size=input_size) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "num_epochs = 25  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(f\"./labeled_images/0001.png\")\n",
    "imgCropped = img.crop(box= (205, 96, 820, 713))\n",
    "enhancer = PIL.ImageEnhance.Contrast(imgCropped)\n",
    "enhanced_image = enhancer.enhance(4.0)\n",
    "gray_image = enhanced_image.convert(\"L\")\n",
    "img = transform(gray_image.convert('RGB'))\n",
    "img = img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f'Predicted class: {predicted.item() + 1}') # Labels for classifier are range 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './outputs/CNN/CNN_classify_attention_patterns.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
