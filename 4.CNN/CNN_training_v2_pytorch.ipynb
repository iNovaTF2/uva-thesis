{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5222fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea4d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename label\n",
       "0  0001.png     3\n",
       "1  0002.png     3\n",
       "2  0003.png     3\n",
       "3  0004.png     1\n",
       "4  0005.png     5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./junk/images_with_labels_v4.csv')\n",
    "data['file_path'] = data['file_path'].apply(lambda x: os.path.basename(x))\n",
    "data['label'] = data['label'].replace(0, 5)\n",
    "data['label'] = data['label'].astype(str)\n",
    "data.rename(columns = {'file_path':'filename'}, inplace = True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419b0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cnn_filename_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f05f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "input_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ea8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform = None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.iloc[index][\"filename\"]\n",
    "        label = self.df.iloc[index][\"label\"] - 1\n",
    "        \n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename)).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e391abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"cnn_filename_labels.csv\", \"./bw_image_test/\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f45aaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778d5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a2d9fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SimpleCNN(nn.Module):\\n    def __init__(self, num_classes, input_size=224):\\n        super(SimpleCNN, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\\n        self.act1 = nn.ReLU()\\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\\n        self.act2 = nn.ReLU()\\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\\n        \\n        # Calculate the size of the flattened features dynamically\\n        size_after_conv = input_size // 4  # Two pooling layers with stride 2\\n        self.fc1 = nn.Linear(32 * size_after_conv * size_after_conv, num_classes)\\n\\n    def forward(self, x):\\n        x = self.pool1(self.act1(self.conv1(x)))\\n        x = self.pool2(self.act2(self.conv2(x)))\\n        x = torch.flatten(x, 1)\\n        x = self.fc1(x)\\n        return x'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=224):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size of the flattened features dynamically\n",
    "        size_after_conv = input_size // 4  # Two pooling layers with stride 2\n",
    "        self.fc1 = nn.Linear(32 * size_after_conv * size_after_conv, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9219958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=input_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 4\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolutional Layer 5\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Adjusted size calculation after 5 pooling layers\n",
    "        size_after_conv = input_size // 32  # Each pooling layer halves the dimension\n",
    "        self.fc1 = nn.Linear(256 * size_after_conv * size_after_conv, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = self.pool3(self.act3(self.conv3(x)))\n",
    "        x = self.pool4(self.act4(self.conv4(x)))\n",
    "        x = self.pool5(self.act5(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ae2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 34.437086092715234%\n",
      "Epoch 2, Accuracy: 36.42384105960265%\n",
      "Epoch 3, Accuracy: 43.04635761589404%\n",
      "Epoch 4, Accuracy: 44.370860927152314%\n",
      "Epoch 5, Accuracy: 54.966887417218544%\n",
      "Epoch 6, Accuracy: 58.94039735099338%\n",
      "Epoch 7, Accuracy: 72.8476821192053%\n",
      "Epoch 8, Accuracy: 74.17218543046357%\n",
      "Epoch 9, Accuracy: 67.54966887417218%\n",
      "Epoch 10, Accuracy: 71.52317880794702%\n",
      "Epoch 11, Accuracy: 73.50993377483444%\n",
      "Epoch 12, Accuracy: 74.17218543046357%\n",
      "Epoch 13, Accuracy: 74.83443708609272%\n",
      "Epoch 14, Accuracy: 63.57615894039735%\n",
      "Epoch 15, Accuracy: 70.86092715231788%\n",
      "Epoch 16, Accuracy: 74.17218543046357%\n",
      "Epoch 17, Accuracy: 69.5364238410596%\n",
      "Epoch 18, Accuracy: 70.19867549668874%\n",
      "Epoch 19, Accuracy: 72.8476821192053%\n",
      "Epoch 20, Accuracy: 73.50993377483444%\n",
      "Epoch 21, Accuracy: 73.50993377483444%\n",
      "Epoch 22, Accuracy: 70.86092715231788%\n",
      "Epoch 23, Accuracy: 70.19867549668874%\n",
      "Epoch 24, Accuracy: 75.49668874172185%\n",
      "Epoch 25, Accuracy: 71.52317880794702%\n",
      "CPU times: user 27min 16s, sys: 7min 18s, total: 34min 34s\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SimpleCNN(num_classes=5, input_size=input_size) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "num_epochs = 25  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb521863",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(f\"./labeled_images/0001.png\")\n",
    "imgCropped = img.crop(box= (205, 96, 820, 713))\n",
    "enhancer = PIL.ImageEnhance.Contrast(imgCropped)\n",
    "enhanced_image = enhancer.enhance(4.0)\n",
    "gray_image = enhanced_image.convert(\"L\")\n",
    "img = transform(gray_image.convert('RGB'))\n",
    "img = img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "126f8e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f'Predicted class: {predicted.item() + 1}') # Labels for classifier are range 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183caeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './CNN_model/ahr_cnn_75_acc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb59bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-cuda-base [Python]",
   "language": "python",
   "name": "conda-env-.conda-cuda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
