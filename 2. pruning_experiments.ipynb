{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link the AHR matrix to be used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './outputs/attention_head_redundancy/boolq_cosine_sim_BERT_1000.pkl'\n",
    "model_link = \"rycecorn/bert-fine-tuned-boolq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and systems check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load redudancy scores from pickle and construct redudancy DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as file:\n",
    "    boolq_cosine_sim_ft = pickle.load(file)\n",
    "\n",
    "# Return average redundancies\n",
    "average_redundancies = [sum(values) / len(values) for values in zip(*boolq_cosine_sim_ft)]\n",
    "\n",
    "# Structure for storing (head1, head2, similarity)\n",
    "num_heads = 144\n",
    "test_store = []\n",
    "avg_idx = 0\n",
    "for i in range(num_heads):\n",
    "    for j in range(i+1, num_heads):\n",
    "      test_store.append([i+1, j+1, average_redundancies[avg_idx]])\n",
    "      avg_idx +=  1\n",
    "        \n",
    "\n",
    "# Dataframe that stores every head vs every other head for analysis\n",
    "\n",
    "# Create matrix with all pairs for analysis\n",
    "original_corr_df = pd.DataFrame(test_store, columns=['head_a', 'head_b', 'similarity'])\n",
    "double_corr_df = pd.DataFrame(test_store, columns=['head_a', 'head_b', 'similarity'])\n",
    "reverse_corr_df = double_corr_df[['head_b', 'head_a', 'similarity']].copy()\n",
    "reverse_corr_df = reverse_corr_df.rename(columns={'head_b': 'head_a', 'head_a': 'head_b'})\n",
    "\n",
    "all_pairs_corr = pd.concat([original_corr_df, reverse_corr_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order head by redundancy scores against all other heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_corr = all_pairs_corr.groupby('head_a').mean().sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary that stores the heads with its corresponding layer index and head index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with any number of heads and layers\n",
    "def get_layers_heads(heads_each_layer, layers_total):\n",
    "    values_layers_heads = {}\n",
    "    total_heads = heads_each_layer * layers_total\n",
    "    \n",
    "    for head in np.arange(1, total_heads+1):\n",
    "        layer = (head - 1) // layers_total + 1\n",
    "        if layer == 1:\n",
    "            values_layers_heads[head] = (layer, head)\n",
    "        else:\n",
    "            head_in_layer = head - (layers_total * (layer - 1))\n",
    "            values_layers_heads[head] = (layer, head_in_layer)\n",
    "        \n",
    "    return values_layers_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get exact heads to prune at each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heads_to_prune(redundant_heads):\n",
    "    # Input is heads ordered by redundancy\n",
    "    prune_ratios = np.arange(0.05, 0.96, 0.05)\n",
    "    number_of_heads = len(redundant_heads)\n",
    "    heads_to_prune_interval = []\n",
    "    \n",
    "    for ratio in prune_ratios:\n",
    "        num_heads_to_prune = round(number_of_heads * ratio)\n",
    "        heads_to_prune_interval.append(redundant_heads[:num_heads_to_prune])\n",
    "    \n",
    "    return heads_to_prune_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch & preprocess the data for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Prepare the batch for DataLoader\n",
    "    input_ids = torch.stack([torch.tensor(d['input_ids']) for d in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(d['attention_mask']) for d in batch])\n",
    "    token_type_ids = torch.stack([torch.tensor(d['token_type_ids']) for d in batch])\n",
    "    labels = torch.tensor([d['labels'] for d in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    # Tokenize the batch of questions and passages\n",
    "    tokenized_data = tokenizer(batch[\"question\"], batch[\"passage\"],\n",
    "                               truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Create labels for the entire batch\n",
    "    tokenized_data['labels'] = [1 if answer else 0 for answer in batch['answer']]\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model link and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model link and tokenizer\n",
    "model_link = \"rycecorn/bert-fine-tuned-boolq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dataset prep\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"boolq\")\n",
    "\n",
    "# Apply the function to the dataset in batches\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "validation_set = tokenized_datasets['validation']\n",
    "validation_loader = DataLoader(validation_set, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create layer and head index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_link)\n",
    "layers = len(model.bert.encoder.layer)\n",
    "heads_each_layer = model.config.num_attention_heads\n",
    "\n",
    "values_layers_heads = get_layers_heads(heads_each_layer, layers)\n",
    "\n",
    "print(f\"The model has {layers} layers with {heads_each_layer} heads in each layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set pruning conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the heads to prune\n",
    "heads_in_order = sorted_avg_corr.index.tolist()\n",
    "pruning_interval_heads = get_heads_to_prune(heads_in_order)\n",
    "\n",
    "# Pruning ratios\n",
    "prune_percentage = np.arange(0.05, 0.96, 0.05)\n",
    "\n",
    "# Creates dictionary of the interval and heads to be pruned for that interval. Has format {'prune_ratio' : [heads_to_be_pruned]}\n",
    "prune_dict = {}\n",
    "i = 0\n",
    "for percentage in prune_percentage:\n",
    "    prune_dict[round(percentage, 2)] = pruning_interval_heads[i]\n",
    "    i += 1\n",
    "\n",
    "print(prune_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heads at each interval for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_items(lists):\n",
    "    result = [pruning_interval_heads[0]]\n",
    "    for i in range(1, len(lists)):\n",
    "        previous_list = lists[i - 1]\n",
    "        current_list = lists[i]\n",
    "        new_items = [item for item in current_list if item not in previous_list]\n",
    "        result.append(new_items)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_heads_each_interval = extract_new_items(pruning_interval_heads)\n",
    "prune_percentage = np.around(np.arange(0.05, 0.96, 0.05), 2)\n",
    "dict_new_heads_each_interval = {}\n",
    "for i in range(0, len(prune_percentage)):\n",
    "    dict_new_heads_each_interval[prune_percentage[i]] = new_heads_each_interval[i]\n",
    "\n",
    "dict_new_heads_each_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a pickle file\n",
    "with open('BERT_BoolQ_heads_pruned.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_new_heads_each_interval, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific data\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "head_size = model.config.hidden_size // num_attention_heads\n",
    "\n",
    "def prune_heads(model, heads_to_prune, head_size):\n",
    "    for head_to_prune in heads_to_prune:\n",
    "        layer_index = values_layers_heads[head_to_prune][0] - 1\n",
    "        head_index = values_layers_heads[head_to_prune][1] - 1\n",
    "\n",
    "        # Zero out the specific head in the query, key, and value matrices\n",
    "        for matrix in ['query', 'key', 'value']:\n",
    "            weight = getattr(model.bert.encoder.layer[layer_index].attention.self, matrix).weight.data\n",
    "            bias = getattr(model.bert.encoder.layer[layer_index].attention.self, matrix).bias.data\n",
    "            weight[:, head_index * head_size:(head_index + 1) * head_size] = 0\n",
    "            bias[head_index * head_size:(head_index + 1) * head_size] = 0\n",
    "\n",
    "def evaluate_model(model, validation_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    final_comp_time = 0\n",
    "\n",
    "    for batch in validation_loader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "        b_token_type_ids = batch['token_type_ids'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        end_time = time.time()\n",
    "        final_comp_time += end_time - start_time\n",
    "        accuracy = (predictions == b_labels).cpu().numpy().mean()\n",
    "        total_eval_accuracy += accuracy\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_loader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_loader)\n",
    "    \n",
    "    return avg_val_accuracy, avg_val_loss, final_comp_time\n",
    "\n",
    "# Main loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pruning_results = []\n",
    "\n",
    "# Load and save the initial state of the model\n",
    "initial_model = AutoModelForSequenceClassification.from_pretrained(model_link)\n",
    "initial_state_dict = initial_model.state_dict()\n",
    "\n",
    "for prune_ratio in tqdm(prune_percentage, desc='Pruning ratios'):\n",
    "    # Load the initial state of the model\n",
    "    model.load_state_dict(initial_state_dict)\n",
    "    \n",
    "    heads_prune_this_iteration = prune_dict[round(prune_ratio, 2)]\n",
    "    prune_heads(model, heads_prune_this_iteration, head_size)\n",
    "    \n",
    "    avg_val_accuracy, avg_val_loss, final_comp_time = evaluate_model(model, validation_loader, device)\n",
    "    \n",
    "    pruning_results.append((round(prune_ratio, 2), len(heads_prune_this_iteration), avg_val_accuracy, final_comp_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case where no heads are pruned\n",
    "# Load and save the initial state of the model\n",
    "initial_model = AutoModelForSequenceClassification.from_pretrained(model_link)\n",
    "initial_state_dict = initial_model.state_dict()\n",
    "\n",
    "# Evaluate the base case where no heads are pruned\n",
    "model.load_state_dict(initial_state_dict)\n",
    "base_accuracy, base_loss, base_comp_time = evaluate_model(model, validation_loader, device)\n",
    "\n",
    "print(f\"Base accuracy: {base_accuracy}\")\n",
    "print(f\"Base inference time: {base_comp_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and save performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pruning & prune evaluation as dataframe\n",
    "pruning_df = pd.DataFrame(pruning_results, columns=['Pruning ratio', 'Num heads pruned', 'Average validation accuracy', 'Comp time'])\n",
    "pruning_df.to_csv('./outputs/pruning/boolq_prune_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pruning_df['Pruning ratio']\n",
    "\n",
    "plt.plot(x, pruning_df['Average validation accuracy'], label='Pruning')\n",
    "plt.title('BoolQ / BERT-base')\n",
    "plt.xlabel('Pruning ratio', fontsize=10)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(np.arange(start=0.05, stop= 1, step=0.05), fontsize=8)\n",
    "plt.axhline(0.7489, color='r', linestyle='--', label='No pruning')\n",
    "plt.grid(True)\n",
    "plt.ylim(0.35, 0.8) # change to your model performance baselines\n",
    "plt.legend()\n",
    "plt.savefig('./outputs/pruning/prune_boolq_BERT-FT.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
